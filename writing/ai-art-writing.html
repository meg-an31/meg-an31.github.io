<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>meg\writing\model collapse</title>
<link rel="stylesheet" href="..\style.css">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
</head>
<body>


<header>
  <pre style="font-family: monospace;" class="ascii-art" align="centre">
                 **=+#-                   
           ##%   .#+=                 
         -+  @     -*#                
      -@=+ #=@      :#*               
    %. -:   :.*      --            *  
  -=  =.    .  %      @@          *#: 
 =:   -     .   :    -::.        *..= 
:+   +.     +   %   =<a href="/writing/"class="hover-link" data-tooltip="writing">%.#</a>+       % : + 
*   <a href="/"class="hover-link" data-tooltip="home">@@</a>     -:   #  -*# @%     :* : .= 
*:#. #.   =-%   *  #.:-*#.   #.    +: 
     .- .#  :. .:  *  *.-:  %      #  
      .-     ::%  +:.  @:: *  ..  =:  
                  + .   ::*  .:   +   
                  + .   -@   -   @    
                  +.    #:  -   @     
                  <a href="/blackout-poetry/"class="hover-link" data-tooltip="blackout poetry">-+</a> -  %  =   %      
                   * - +. ..  #       
                   # . @  -  #        
                   =..:= +  #         
                    # %. : *          
                    - = : #           
                     %: #*:           
                     :-.:+            
                      *#%.            
                       %@             
  </pre>
</header>

<div class="main-layout">
  
  <div class="side-rectangle left-side">
    <pre class="side-ascii" style="font-family: monospace;">
      <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>



    </pre>
  </div>
  
  <div class="content">
    <h1>in defence of the human artist</h1>
    <h2>model collapse and the limits of artificial inspiration</h2>
    <h4>this was written for the oxford scientist's tt25 mag <i>limits</i>. i don't know how much i like it, but it was a good learning experience, so it's here for archival purposes.</h4>
    <br><p>The fear of Artificial Intelligence usurping humanity is growing. Over the past few years, we have seen a rapid increase in impressive AI-generated poetry, prose, visual art, and film, often which is indistinguishable (and sometimes preferred) to that created by humans. Such progress is owing to the models' ability to learn from vast amounts of human data, inferring patterns from our innovation. But what happens when these models learn only from themselves? Here, a problem arises, quietly emerging alongside the explosion of AI artwork: a degeneration of quality when models use AI-generated inputs as training data, aptly named model collapse.</p>
    <br><p>To understand exactly why model collapse happens, think of an AI system as a huge statistical function which is built to represent the underlying patterns in a set of data. For something as complex as natural language or art, this function can never be made perfect. Teaching a machine about our infinitely complex world using a finite number of parameters is an impossible task. Nonetheless, we can build something resembling understanding by giving the machine a collection of examples to learn from (named the training dataset). For every example fed to the function, we evaluate its success at predicting the ideal output, adjusting the internal parameters as needed. Once all of the training data has been used, the output we receive is similar to what we might naturally expect, only with three specific sources of error. First, the training data is finite, so some information is never shown to the model from the outset. The second error arises because the function has a fixed number of parameters, meaning that the amount it can express is limited. Therefore, our model cannot capture every subtlety of the real data, even with a theoretically perfect set of training data. Finally, the calculations of internal parameters of the model are done by learning algorithms, which are, at their core, approximations, hence introduce their own errors.</p>
    <br><p>Model collapse occurs because these errors are amplified when a model learns from itself. The imperfections from the first approximation are encoded into the outputs it gives, and so any attempt to learn from itself results in an echo chamber of compounding error. The result is that model quality degrades, until it has entirely forgotten the nuance and unpredictability of the original data, eventually reaching a state of utter uselessness. The solution might seem clear: if AI generated data is poisonous to these models, then don’t train models on AI-generated data. But this is not as easy as it seems.</p>
      
    <br><p>Take the example of the first few iterations of ChatGPT and other large language models (LLMs), which were trained partly on text from the internet. Before these models exploded in popularity, the internet was a place rife with unpredictable and innovative content produced by a diverse set of people. It was characteristically human— a uniquely ideal set of training data. Yet these LLMs have now existed for so long that it is near impossible to carve out online space free from their artificial content. If developers want to continue using the internet to train their models, they must accept the risk that AI-generated articles, conversations, and art are always going to be present, causing this input to lose some of its inherent richness and variety. “More training data results in a better model” was a ground truth which guided the development of LLMs to their current impressive state. Now this is no longer able to be relied upon, the training process becomes less efficient and more expensive. Training a model to act creatively displays a particularly rapid decline in diversity of output. Though some experts speculate that the internet can still be a rich and useful source of training data, even with this explosion of AI-generated content, so long as there is also an equally rapid increase in human data.</p>
      
    <br><p>So, where does all of this leave the human artist? Rhetoric recently would have you believe that human art is losing value, with books, art, and films being produced entirely artificially. It is hard to ignore that AI art is more efficient, economically viable, and sometimes better quality than something made by a human. Nonetheless, model collapse is compelling evidence that reveals the intrinsic power of the human artist—the ability to create perpetually, without a degeneration of quality. Our irreplicable experience of emotion, temporal memory, physical sensation, and the randomness of daily life gives us something machines clearly lack: the ability to be inspired. Model collapse presents us with both a technical problem of mathematical approximation, and a philosophical indicator pointing to the infinite complexity of human art. AI is a tool powerful beyond comprehension, but it cannot sustain itself without true human innovation.</p>
    <br>
    <p style="text-align: right;">~/05/25</p>
    <br><br><br>
  </div>

  
  
  <div class="side-rectangle right-side">
    <pre class="side-ascii" style="font-family: monospace;">
      <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
      
   *   '*
           *
                *
                       *
               *
                     *



    </pre>
  </div>
</div>

<script src="../script.js"></script>
</body>
</html>
